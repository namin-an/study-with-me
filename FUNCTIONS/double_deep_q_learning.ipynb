{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"double_deep_q_learning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPPbc2H/PJDv9bkrTOz6IGb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["References:\n","- https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html\n","- https://somjang.tistory.com/entry/Google-Colab%EC%97%90%EC%84%9C-OpenAI-gym-render-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95"],"metadata":{"id":"KWkoJejostod"}},{"cell_type":"markdown","source":["# Importing packages"],"metadata":{"id":"5VzxrsWRmTrO"}},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7UMtyEA6sg6n","executionInfo":{"status":"ok","timestamp":1648558038782,"user_tz":-540,"elapsed":1928,"user":{"displayName":"Na Min An","userId":"07779782386374796484"}},"outputId":"3783655f-2581-415e-afef-16d8a251c996"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import os\n","os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Rq8F8Va7uLk4","executionInfo":{"status":"ok","timestamp":1648558040893,"user_tz":-540,"elapsed":313,"user":{"displayName":"Na Min An","userId":"07779782386374796484"}},"outputId":"405bb53c-6ec6-4cf7-c72b-981d3a616fad"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["! pip install nes-py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kpeJVI_42xmd","executionInfo":{"status":"ok","timestamp":1648554854333,"user_tz":-540,"elapsed":16066,"user":{"displayName":"Na Min An","userId":"07779782386374796484"}},"outputId":"ec331bca-2e1d-4e0e-b41e-d5733792d284"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nes-py\n","  Downloading nes_py-8.1.8.tar.gz (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 1.4 MB/s \n","\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.21.5)\n","Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.5.0)\n","Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (4.63.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py) (0.16.0)\n","Building wheels for collected packages: nes-py\n","  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nes-py: filename=nes_py-8.1.8-cp37-cp37m-linux_x86_64.whl size=437914 sha256=9b62bec8a95767c2312644abd9490edf8ab6e956aa9850f236413c1711acd54e\n","  Stored in directory: /root/.cache/pip/wheels/f2/05/1f/608f15ab43187096eb5f3087506419c2d9772e97000f3ba025\n","Successfully built nes-py\n","Installing collected packages: nes-py\n","Successfully installed nes-py-8.1.8\n"]}]},{"cell_type":"code","source":["!pip install gym-super-mario-bros==7.3.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bSkrd0Ad202S","executionInfo":{"status":"ok","timestamp":1648554859360,"user_tz":-540,"elapsed":5034,"user":{"displayName":"Na Min An","userId":"07779782386374796484"}},"outputId":"88adb6e9-ee8f-4625-d8ce-6499bee44c68"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gym-super-mario-bros==7.3.0\n","  Downloading gym_super_mario_bros-7.3.0-py2.py3-none-any.whl (198 kB)\n","\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 30 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 198 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: nes-py>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from gym-super-mario-bros==7.3.0) (8.1.8)\n","Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.5.0)\n","Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.17.3)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.21.5)\n","Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (4.63.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.16.0)\n","Installing collected packages: gym-super-mario-bros\n","Successfully installed gym-super-mario-bros-7.3.0\n"]}]},{"cell_type":"code","source":["import time, datetime\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch import nn\n","from torchvision import transforms as T\n","from PIL import Image\n","import numpy as np\n","from pathlib import Path\n","from collections import deque\n","import random, datetime, os, copy\n","\n","# Gym is an OpenAI toolkit for RL\n","import gym\n","from gym.spaces import Box\n","from gym.wrappers import FrameStack\n","\n","# NES Emulator for OpenAI Gym\n","from nes_py.wrappers import JoypadSpace\n","\n","# Super Mario environment for OpenAI Gym\n","import gym_super_mario_bros"],"metadata":{"id":"XW5C4YDa2Z3D","executionInfo":{"status":"ok","timestamp":1648558477441,"user_tz":-540,"elapsed":226,"user":{"displayName":"Na Min An","userId":"07779782386374796484"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# Initializing and preprocessing environment"],"metadata":{"id":"5Kum2_-CmZS1"}},{"cell_type":"code","source":["class SkipFrame(gym.Wrapper):\n","  \"\"\"\n","  Skipping n-intermediate frames w/ losing much information.\n","  \"\"\"\n","  def __init__(self, env, skip):\n","    \"\"\"Returning only every 'skip'-th frame\"\"\"\n","    super().__init__(env)\n","    self._skip = skip\n","\n","  def step(self, action):\n","    \"\"\"Summing up reward with the given action\"\"\"\n","    total_reward = 0.0\n","    done = False\n","    for i in range(self._skip):\n","      # Acumulate reward and repeat the same action\n","      obs, reward, done, info = self.env.step(action)\n","      total_reward += reward # stands for the \"quality\" of the action in a state\n","      if done:\n","        break\n","    return obs, total_reward, done, info\n","\n","class GrayScaleObservation(gym.ObservationWrapper):\n","    \"\"\"\n","    Turning environment into gray-scaled space.\n","    \"\"\"\n","    def __init__(self, env):\n","        super().__init__(env)\n","        obs_shape = self.observation_space.shape[:2]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def permute_orientation(self, observation):\n","        \"\"\"permute [H, W, C] array to [C, H, W] tensor\"\"\"\n","        observation = np.transpose(observation, (2, 0, 1))\n","        observation = torch.tensor(observation.copy(), dtype=torch.float)\n","        return observation\n","\n","    def observation(self, observation):\n","        \"\"\"return grayscaled observation space\"\"\"\n","        observation = self.permute_orientation(observation)\n","        transform = T.Grayscale()\n","        observation = transform(observation)\n","        return observation\n","\n","class ResizeObservation(gym.ObservationWrapper):\n","    \"\"\"\n","    Resizing environment to the given shape\n","    \"\"\"\n","    def __init__(self, env, shape):\n","        super().__init__(env)\n","        if isinstance(shape, int):\n","            self.shape = (shape, shape)\n","        else: # if shape is not an integer\n","            self.shape = tuple(shape)\n","        \n","        obs_shape = self.shape + self.observation_space.shape[2:]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","    \n","    def observation(self, observation):\n","        \"\"\"return resized and normalized space\"\"\"\n","        transforms = T.Compose([T.Resize(self.shape), T.Normalize(0, 255)])\n","        observation = transforms(observation).squeeze(0)\n","        return observation"],"metadata":{"id":"88ipoY4Nsyig","executionInfo":{"status":"ok","timestamp":1648559722315,"user_tz":-540,"elapsed":381,"user":{"displayName":"Na Min An","userId":"07779782386374796484"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["# Creating agent Mario for reinforcement learning"],"metadata":{"id":"RFYZhpw8mdhp"}},{"cell_type":"code","source":["# Creating agent's DNN\n","class MarioNet(nn.Module):\n","    \"\"\"\n","    Creating mini cnn structure for Mario\n","    \"\"\"\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        c, h, w = input_dim\n","        \n","        if h != 84:\n","            raise ValueError(f'Expecting input height: 84, got : {h}')\n","        if w != 84:\n","            raise ValueError(f'Expecting input height: 84, got : {w}')\n","\n","        self.online = nn.Sequential(\n","            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(3136, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, output_dim),\n","        )\n","\n","        self.target = copy.deepcopy(self.online)\n","\n","        # Freeze parameters for target DQN\n","        for p in self.target.parameters():\n","            p.requires_grad = False\n","        \n","    def forward(self, input, model):\n","        if model == 'online':\n","            return self.online(input)\n","        elif model == 'target':\n","            return self.target(input)\n","\n","\n","# Creating agent\n","class Mario:\n","    \"\"\"\n","    Creating Mario that can act wisely by choosing the most optimal action (exploit) or takes a random action (explore).\n","    \"\"\"\n","    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","\n","        # cache and recall\n","        self.memory = deque(maxlen=10000)\n","\n","        # learn\n","        self.batch_size = 32\n","        self.exploration_rate = 1\n","        self.exploration_rate_decay = 0.99999975\n","        self.exploration_rate_min = 0.1\n","        self.gamma = 0.9\n","\n","        self.curr_step = 0\n","        self.burnin = 1e4 # minimum no. of experiences before training\n","        self.learn_every = 3 # no. of experiences between updates to Q_online\n","        self.sync_every = 1e4 # no. of experiences between Q_target & Q_online sync\n","\n","        self.save_every = 5 #5e5 # no. of experiences between saving MarioNet\n","        self.save_dir = save_dir\n","\n","        self.use_cuda = torch.cuda.is_available() # True\n","        \n","        # Mario's DNN to predict the most optimal action\n","        self.net = MarioNet(self.state_dim, self.action_dim).float()\n","        if self.use_cuda:\n","            self.net = self.net.to(device='cuda')\n","        if checkpoint:\n","            self.load(checkpoint)\n","\n","        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n","        self.loss_fn = torch.nn.SmoothL1Loss()\n","        \n","      \n","    def act(self, state):\n","        \"\"\"perform a greedy action based on epsilon value given a state\"\"\"\n","        # EXPLORE\n","        if np.random.randn() < self.exploration_rate:\n","            action_idx = np.random.randint(self.action_dim)\n","        # EXPLOIT\n","        else:        \n","            state = state.__array__()\n","            state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n","            state = state.unsqueeze(0)\n","            action_values = self.net(state, model='online')\n","            action_idx = torch.argmax(action_values, axis=1).item()\n","            \n","        # decrease exploration rate\n","        self.exploration_rate *= self.exploration_rate_decay\n","        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n","        \n","        # increment step\n","        self.curr_step += 1\n","        return action_idx  \n","        \n","    def cache(self, state, next_state, action, reward, done): # experiences\n","        \"\"\"stores experiences to memory (replay buffer)\"\"\"\n","        state = state.__array__()\n","        next_state = next_state.__array__()\n","        \n","        state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n","        next_state = torch.FloatTensor(next_state).cuda() if self.use_cuda else torch.FloatTensor(state)\n","        action = torch.LongTensor([action]).cuda() if self.use_cuda else torch.LongTensor(state)\n","        reward = torch.DoubleTensor([reward]).cuda() if self.use_cuda else torch.DoubleTensor(state)\n","        done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor(state)\n","\n","        self.memory.append((state, next_state, action, reward, done,))\n","    \n","    def recall(self):\n","        \"\"\"retrieves a batch of experiences from memory\"\"\"\n","        batch = random.sample(self.memory, self.batch_size)\n","        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n","        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n","\n","    def td_estimate(self, state, action):\n","        \"\"\"predict optimal Q-value for a given state\"\"\"\n","        current_Q = self.net(state, model='online')[np.arange(0, self.batch_size), action] # Q_online(s, a)\n","        return current_Q\n","      \n","    @torch.no_grad()\n","    def td_target(self, reward, next_state, done):\n","        \"\"\"aggregate current reward and the estimated Q-values for the next state\"\"\"\n","        next_state_Q = self.net(next_state, model='online')\n","        best_action = torch.argmax(next_state_Q, axis=1) # precalculating action value for the next state\n","        next_Q = self.net(next_state, model='target')[np.arange(0, self.batch_size), best_action]\n","        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n","      \n","    def update_Q_online(self, td_estimate, td_target):\n","        \"\"\"backpropagate online dqn\"\"\"\n","        loss = self.loss_fn(td_estimate, td_target)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        return loss.item()\n","\n","    def sync_Q_target(self):\n","        \"\"\"copy online dqn to target dqn\"\"\"\n","        self.net.target.load_state_dict(self.net.online.state_dict()) \n","\n","    def save(self):\n","        \"\"\"save checkpoint\"\"\"\n","        save_path = (self.save_dir / f'mario_net_{int(self.curr_step // self.save_every)}.chkpt')\n","        torch.save(dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate), save_path)\n","        print(f'MarioNet saved to {save_path} at step {self.curr_step}')\n","\n","    def load(self, load_path):\n","        \"\"\"load model with saved weights\"\"\"\n","        if not load_path.exists():\n","            raise ValueError(f\"{load_path} does not exist\")\n","\n","        ckp = torch.load(load_path, map_location=('cuda' if self.use_cuda else 'cpu'))\n","        exploration_rate = ckp.get('exploration_rate')\n","        state_dict = ckp.get('model')\n","\n","        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n","        self.net.load_state_dict(state_dict)\n","        self.exploration_rate = exploration_rate\n","    \n","    def learn(self):\n","        \"\"\"update online action value (Q) function with a batch of experiences\"\"\"\n","        # ready to update target DQN\n","        if self.curr_step % self.sync_every == 0:\n","            self.sync_Q_target()\n","\n","        # ready to save\n","        if self.curr_step % self.save_every == 0:\n","            self.save()\n","        \n","        # not ready to train\n","        if self.curr_step < self.burnin:\n","            return None, None\n","        \n","        # sample from memory bank\n","        state, next_state, action, reward, done = self.recall()\n","\n","        # get Q-values of online dqn\n","        td_est = self.td_estimate(state, action)\n","\n","        # get Q-values of target dqn\n","        td_tgt = self.td_target(reward, next_state, done)\n","\n","        # backpropagate loss through Q_online\n","        loss = self.update_Q_online(td_est, td_tgt)\n","\n","        return (td_est.mean().item(), loss)"],"metadata":{"id":"PHnkovvzhsxl","executionInfo":{"status":"ok","timestamp":1648565671432,"user_tz":-540,"elapsed":1209,"user":{"displayName":"Na Min An","userId":"07779782386374796484"}}},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":["# Logging to save training information"],"metadata":{"id":"iCkEcHJiwSFG"}},{"cell_type":"code","source":["class MetricLogger:\n","    def __init__(self, save_dir):\n","        self.save_log = save_dir / \"log\"\n","        with open(self.save_log, \"w\") as f:\n","            f.write(\n","                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n","                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n","                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n","            )\n","        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n","        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n","        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n","        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n","\n","        # History metrics\n","        self.ep_rewards = []\n","        self.ep_lengths = []\n","        self.ep_avg_losses = []\n","        self.ep_avg_qs = []\n","\n","        # Moving averages, added for every call to record()\n","        self.moving_avg_ep_rewards = []\n","        self.moving_avg_ep_lengths = []\n","        self.moving_avg_ep_avg_losses = []\n","        self.moving_avg_ep_avg_qs = []\n","\n","        # Current episode metric\n","        self.init_episode()\n","\n","        # Timing\n","        self.record_time = time.time()\n","\n","    def log_step(self, reward, loss, q):\n","        self.curr_ep_reward += reward\n","        self.curr_ep_length += 1\n","        if loss:\n","            self.curr_ep_loss += loss\n","            self.curr_ep_q += q\n","            self.curr_ep_loss_length += 1\n","\n","    def log_episode(self):\n","        \"Mark end of episode\"\n","        self.ep_rewards.append(self.curr_ep_reward)\n","        self.ep_lengths.append(self.curr_ep_length)\n","        if self.curr_ep_loss_length == 0:\n","            ep_avg_loss = 0\n","            ep_avg_q = 0\n","        else:\n","            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n","            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n","        self.ep_avg_losses.append(ep_avg_loss)\n","        self.ep_avg_qs.append(ep_avg_q)\n","\n","        self.init_episode()\n","\n","    def init_episode(self):\n","        self.curr_ep_reward = 0.0\n","        self.curr_ep_length = 0\n","        self.curr_ep_loss = 0.0\n","        self.curr_ep_q = 0.0\n","        self.curr_ep_loss_length = 0\n","\n","    def record(self, episode, epsilon, step):\n","        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n","        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n","        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n","        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n","        self.moving_avg_ep_rewards.append(mean_ep_reward)\n","        self.moving_avg_ep_lengths.append(mean_ep_length)\n","        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n","        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n","\n","        last_record_time = self.record_time\n","        self.record_time = time.time()\n","        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n","\n","        print(\n","            f\"Episode {episode} - \"\n","            f\"Step {step} - \"\n","            f\"Epsilon {epsilon} - \"\n","            f\"Mean Reward {mean_ep_reward} - \"\n","            f\"Mean Length {mean_ep_length} - \"\n","            f\"Mean Loss {mean_ep_loss} - \"\n","            f\"Mean Q Value {mean_ep_q} - \"\n","            f\"Time Delta {time_since_last_record} - \"\n","            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n","        )\n","\n","        with open(self.save_log, \"a\") as f:\n","            f.write(\n","                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n","                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n","                f\"{time_since_last_record:15.3f}\"\n","                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n","            )\n","\n","        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n","            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n","            plt.savefig(getattr(self, f\"{metric}_plot\"))\n","            plt.clf()"],"metadata":{"id":"MgwAe3dYwRb9","executionInfo":{"status":"ok","timestamp":1648565144523,"user_tz":-540,"elapsed":239,"user":{"displayName":"Na Min An","userId":"07779782386374796484"}}},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":["# MAIN"],"metadata":{"id":"xn14RYWvwI-z"}},{"cell_type":"code","source":["# if __name__ == \"__main__\":\n","\n","# Initializing environment\n","env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\") # initialize Super Mario environment\n","env = JoypadSpace(env, [['right'], ['right', 'A']]) # limit the action-space to 0. walk right and 1. jump right\n","env.reset()\n","\n","next_state, reward, done, info = env.step(action=0)\n","print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")\n","\n","# Preprocessing environment\n","env = SkipFrame(env, skip=4)\n","env = GrayScaleObservation(env)\n","env = ResizeObservation(env, shape=84)\n","env = FrameStack(env, num_stack=4) # four gray-scaled (84, 84) consecutive frames stacked states\n","\n","# Training agent for at least 40,000 episodes\n","use_cuda = torch.cuda.is_available()\n","print(f\"Using CUDA: {use_cuda}\\n\")\n","save_dir = Path('/content/gdrive/MyDrive/checkpoints') / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n","save_dir.mkdir(parents=True) #, exist_ok=True)\n","\n","evaluation = True # either train or replay\n","if evaluation:\n","    checkpoint_file = Path('/content/gdrive/MyDrive/checkpoints/trained_mario.chkpt') \n","else:\n","    checkpoint_file = None\n","mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir, checkpoint=checkpoint_file)\n","if evaluation:\n","    mario.exploration_rate = mario.exploration_rate_min\n","logger = MetricLogger(save_dir)\n","episodes = 10 #4000\n","for e in range(episodes):\n","    state = env.reset()\n","\n","    # Play the game!\n","    while True:\n","        if evaluation:\n","          # show environment\n","          env.render()\n","        # Run agent on the state\n","        action = mario.act(state)\n","        # Return experiences based on agent's action\n","        next_state, reward, done, info = env.step(action)\n","        # remember\n","        mario.cache(state, next_state, action, reward, done)\n","        if evaluation:\n","            logger.log_step(reward, None, None)\n","        else:\n","            # learn Q-values for agent\n","            q, loss = mario.learn()\n","            # log saved Q-values\n","            logger.log_step(reward, loss, q)\n","        # update state\n","        state = next_state\n","        # check if the game ended\n","        if done or info['flag_get']:\n","            break\n","    \n","    logger.log_episode()\n","\n","    if e % 20 == 0:\n","        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":534},"id":"T9MsO6Apvlrj","executionInfo":{"status":"error","timestamp":1648565941205,"user_tz":-540,"elapsed":1546,"user":{"displayName":"Na Min An","userId":"07779782386374796484"}},"outputId":"f4ff7ae3-2cdc-4d9e-87da-cb428accda61"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["(240, 256, 3),\n"," 0,\n"," False,\n"," {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n","Using CUDA: True\n","\n","Loading model at /content/gdrive/MyDrive/checkpoints/trained_mario.chkpt with exploration rate 0.1\n"]},{"output_type":"error","ename":"NoSuchDisplayException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-96-89e49e1ae723>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0;31m# show environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m           \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Run agent on the state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nes_py/nes_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    377\u001b[0m                 )\n\u001b[1;32m    378\u001b[0m             \u001b[0;31m# show the screen on the image viewer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nes_py/_image_viewer.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# open the window if it isn't open already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;31m# prepare the window for the next frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nes_py/_image_viewer.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mvsync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mresizable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         )\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""]}]},{"cell_type":"code","source":[""],"metadata":{"id":"ovHUEiJLGzZH"},"execution_count":null,"outputs":[]}]}